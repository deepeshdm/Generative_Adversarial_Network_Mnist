{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_on_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzNZFHyIAPTN"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from keras.layers import Conv2D,Conv2DTranspose,ReLU,LeakyReLU,Reshape,\\\n",
        "    BatchNormalization,Dropout,Flatten,Dense,Activation,UpSampling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyCyNzyuAcI6"
      },
      "source": [
        "\n",
        "def discriminator():\n",
        "    net = Sequential()\n",
        "    input_shape = (28, 28, 1)\n",
        "    dropout_prob = 0.5\n",
        "\n",
        "    net.add(Conv2D(64,kernel_size=4,strides=2, input_shape=input_shape, padding='same'))\n",
        "    net.add(LeakyReLU())\n",
        "    net.add(BatchNormalization())\n",
        "\n",
        "    net.add(Conv2D(128,kernel_size=4, strides=2, padding='same'))\n",
        "    net.add(LeakyReLU())\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(Dropout(dropout_prob))\n",
        "\n",
        "    net.add(Conv2D(256,kernel_size=4, strides=2, padding='same'))\n",
        "    net.add(LeakyReLU())\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(Dropout(dropout_prob))\n",
        "\n",
        "    net.add(Conv2D(512,kernel_size=4, strides=2, padding='same'))\n",
        "    net.add(LeakyReLU())\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(Dropout(dropout_prob))\n",
        "\n",
        "    net.add(Flatten())\n",
        "    net.add(Dense(1))\n",
        "    net.add(Activation('sigmoid'))\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def generator():\n",
        "    net = Sequential()\n",
        "    dropout_prob = 0.5\n",
        "\n",
        "    net.add(Dense(7 * 7 * 256, input_dim=100))\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(ReLU())\n",
        "    net.add(Reshape((7, 7, 256)))\n",
        "    net.add(Dropout(dropout_prob))\n",
        "\n",
        "    net.add(UpSampling2D(size=(2,2)))\n",
        "    net.add(Conv2D(128, kernel_size=(2, 2), strides=2))\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(ReLU())\n",
        "\n",
        "    net.add(UpSampling2D(size=(2, 2)))\n",
        "    net.add(Conv2D(64, kernel_size=(2, 2), strides=2))\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(ReLU())\n",
        "\n",
        "    net.add(UpSampling2D(size=(2, 2)))\n",
        "    net.add(Conv2D(32, 2, padding='same'))\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(ReLU())\n",
        "\n",
        "    net.add(UpSampling2D(size=(2, 2)))\n",
        "    net.add(Conv2D(16, 2, padding='same'))\n",
        "    net.add(BatchNormalization())\n",
        "    net.add(ReLU())\n",
        "\n",
        "    net.add(Conv2D(1, 2, padding='same'))\n",
        "    net.add(Activation('tanh'))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SaiDkVuAfjT"
      },
      "source": [
        "\n",
        "# We create & add the discriminator network to a new Sequential\n",
        "# model and do not directly compile the discriminator itself.\n",
        "\n",
        "net_discriminator = discriminator()\n",
        "optim_discriminator = Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "model_discriminator = Sequential()\n",
        "model_discriminator.add(net_discriminator)\n",
        "model_discriminator.compile(loss='binary_crossentropy', optimizer=optim_discriminator, metrics=['accuracy'])\n",
        "\n",
        "model_discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP-2cQgGAioR"
      },
      "source": [
        "# We add both networks to a combined model i.e adversarial model.\n",
        "# Also we freeze the discriminator in this model.\n",
        "\n",
        "net_generator = generator()\n",
        "optim_adversarial = Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "model_adversarial = Sequential()\n",
        "model_adversarial.add(net_generator)\n",
        "\n",
        "# Disable layers in discriminator\n",
        "for layer in net_discriminator.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_adversarial.add(net_discriminator)\n",
        "model_adversarial.compile(loss='binary_crossentropy', optimizer=optim_adversarial, metrics=['accuracy'])\n",
        "\n",
        "model_adversarial.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rkeFSEjAmuS"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# scaling x_train between\n",
        "x_train = x_train/255\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bUrBGtUApOL"
      },
      "source": [
        "# Training the DCGAN model\n",
        "batch_size = 128\n",
        "iter = 10000\n",
        "\n",
        "for i in range(iter):\n",
        "\n",
        "    # Select a random set of training images from the mnist dataset\n",
        "    images_train = x_train[np.random.randint(0, x_train.shape[0], size=batch_size), :, :, :]\n",
        "    # Generate a random noise vector\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "    # Use the generator to create fake images from the noise vector\n",
        "    images_fake = net_generator.predict(noise)\n",
        "\n",
        "    # Create a dataset with fake and real images\n",
        "    x = np.concatenate((images_train, images_fake))\n",
        "    y = np.ones([2 * batch_size, 1])\n",
        "    y[batch_size:, :] = 0\n",
        "\n",
        "    # Train discriminator for one batch\n",
        "    d_stats = model_discriminator.train_on_batch(x, y)\n",
        "\n",
        "    # Train the adversarial model for one batch\n",
        "    y = np.ones([batch_size, 1])\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "    a_stats = model_adversarial.train_on_batch(noise, y)\n",
        "\n",
        "\n",
        "    if i%20==0:\n",
        "      print(\"{}% task complete...\".format(round((i/iter)*100,2)))\n",
        "\n",
        "      # Plotting images generated by Generator\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      plt.figure(figsize=(15,6))\n",
        "      noise = np.random.uniform(-1.0, 1.0, size=[40, 100])\n",
        "      images = net_generator.predict(noise)\n",
        "\n",
        "      for i in range(40):\n",
        "         image = images[i, :, :, :]\n",
        "         image = np.reshape(image, [28, 28])\n",
        "         plt.subplot(4, 10, i+1)\n",
        "         plt.imshow(image, cmap='gray')\n",
        "         plt.axis('off')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}